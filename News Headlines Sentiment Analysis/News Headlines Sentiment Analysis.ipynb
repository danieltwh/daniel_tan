{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news_headlines_train.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In addition , a further 29 employees can be la...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The authorisation is in force until the end of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The value of the deal was not disclosed .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You need to be ready when the window opens up ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Major Order in India Comptel Corporation has r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  In addition , a further 29 employees can be la...         -1\n",
       "1  The authorisation is in force until the end of...          0\n",
       "2          The value of the deal was not disclosed .          0\n",
       "3  You need to be ready when the window opens up ...          0\n",
       "4  Major Order in India Comptel Corporation has r...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3193"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1898\n",
       " 1     908\n",
       "-1     387\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = df.sentiment.value_counts(normalize = True) *100\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "# plt.figure(figsize=(12,6))\n",
    "\n",
    "sns.barplot(x=counts.index,y=counts, ax=ax)\n",
    "\n",
    "ax.set_title(\"Sentiment Scores of News Headlines\")\n",
    "ax.set_xlabel(\"Sentiment Scores\")\n",
    "ax.set_ylabel(\"Percentage (%)\");\n",
    "\n",
    "# plt.title(\"Sentiment Scores of News Headlines\")\n",
    "# plt.xlabel(\"Sentiment Scores\")\n",
    "# plt.ylabel(\"Percentage (%)\")\n",
    "# plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for headlines in df.text:\n",
    "    # Lowercase the headlines\n",
    "    headlines = headlines.lower()\n",
    "    # Removing punctuations\n",
    "    headlines = re.sub(\"[,.®'&$’\\\"\\-()]\", \" \", headlines)\n",
    "    temp.append(word_tokenize(headlines))\n",
    "df[\"Tokens\"] = temp\n",
    "\n",
    "# Getting the length of the headlines\n",
    "df[\"Length\"] = df[\"Tokens\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "sns.distplot(df[\"Length\"], kde = False)\n",
    "\n",
    "\n",
    "plt.title(\"Text Length of News Article\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Number of Headlines\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"sentiment\"], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r\"[a-zA-Z0-9]+\")\n",
    "cv = CountVectorizer(stop_words=\"english\", tokenizer=token.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts = cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model1 = MultinomialNB()\n",
    "mnb_model1.fit(text_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = mnb_model1.predict(test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score1 = accuracy_score(pred1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7118997912317327\n"
     ]
    }
   ],
   "source": [
    "print(acc_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes accuracy seems to be performing okay, but not that great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df = pd.read_csv(\"news_headlines_test_sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Proline Plus is available in both adjustable s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Digia said its consolidated net sales for Janu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cash flow from operating activities is estimat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11 August 2010 - Finnish measuring equipment m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metso Foundries Jyvaskyla Oy will discontinue ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  Proline Plus is available in both adjustable s...          0\n",
       "1  Digia said its consolidated net sales for Janu...          0\n",
       "2  Cash flow from operating activities is estimat...          0\n",
       "3  11 August 2010 - Finnish measuring equipment m...          0\n",
       "4  Metso Foundries Jyvaskyla Oy will discontinue ...          0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model3 = MultinomialNB()\n",
    "\n",
    "# Training model with the entire dataset\n",
    "text_counts3 = cv.fit_transform(df[\"text\"])\n",
    "mnb_model3.fit(text_counts3, df[\"sentiment\"])\n",
    "\n",
    "# Transforming submission dataset\n",
    "subm_X1 = cv.transform(subm_df[\"text\"])\n",
    "\n",
    "# Making predictions for submission dataset\n",
    "pred3 = mnb_model3.predict(subm_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting\n",
    "subm_df['sentiment'] = np.array(pred3)\n",
    "subm_df.to_csv('Subm_naivebayes1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Daniel/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vader_score\"] = df[\"text\"].apply(lambda x: sid.polarity_scores(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"compound\"] = df[\"vader_score\"].apply(lambda x: x[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_label(score):\n",
    "    if score > 0:\n",
    "        return 1\n",
    "    elif score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted labels from compound score\n",
    "df[\"vader_pred\"] = df[\"compound\"].apply(score_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_acc = accuracy_score(df[\"sentiment\"], df[\"vader_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5537112433448168\n"
     ]
    }
   ],
   "source": [
    "print(vader_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like VADER sentiment analysis is not performing well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to run VADER\n",
    "def vader_sentiment(column):\n",
    "    vader_score = column.apply(lambda x: sid.polarity_scores(x))\n",
    "    compound= vader_score.apply(lambda x: x[\"compound\"])\n",
    "    vader_pred = compound.apply(score_label)\n",
    "    return vader_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df = pd.read_csv(\"news_headlines_test_sample_submission.csv\")\n",
    "subm_df['sentiment'] = vader_sentiment(subm_df[\"text\"])\n",
    "\n",
    "# Exporting\n",
    "subm_df.to_csv('Subm_vader1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with BERT\n",
    "Since there are 3 labels for the sentiment, we will be using a Multiclass Text Classification BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for the BERT Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'addition', ',', 'a', 'further', '29', 'employees', 'can', 'be', 'laid', 'off', 'until', 'further', 'notice', 'and', 'the', 'whole', 'workforce', 'can', 'be', 'laid', 'off', 'for', 'short', 'periods', 'if', 'needed', '.']\n",
      "[1107, 1901, 117, 170, 1748, 1853, 4570, 1169, 1129, 3390, 1228, 1235, 1748, 4430, 1105, 1103, 2006, 17034, 1169, 1129, 3390, 1228, 1111, 1603, 6461, 1191, 1834, 119]\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = [tokenizer.tokenize(text) for text in df[\"text\"]]\n",
    "\n",
    "ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]\n",
    "\n",
    "labels = df[\"sentiment\"].values\n",
    "\n",
    "print(tokenized_text[0])\n",
    "\n",
    "print(ids[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "max_len = len(ids[0])\n",
    "for i in ids:\n",
    "    if(len(i)>max_len):\n",
    "        max_len = len(i)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1107  1901   117 ...     0     0     0]\n",
      " [ 1103  2351  5771 ...     0     0     0]\n",
      " [ 1103  2860  1104 ...     0     0     0]\n",
      " ...\n",
      " [  189 12577  2497 ...     0     0     0]\n",
      " [  185  5822  2211 ...     0     0     0]\n",
      " [ 1104  1736   117 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = pad_sequences(ids, maxlen= max_len, dtype = \"long\",\\\n",
    "                         truncating = \"post\", padding = \"post\")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a label converter (using -1 encounter an error when training)\n",
    "# Negative --> 0\n",
    "# Neutral --> 1\n",
    "# Positive --> 2\n",
    "\n",
    "def label_converter(label):\n",
    "    if label == -1:\n",
    "        return 0\n",
    "    elif label == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels\"] = df[\"sentiment\"].apply(label_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"labels\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training (90%) and validation sets (10%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/Daniel/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Tokenising the text for training and validation sets\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    X_train.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length = 64,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    X_test.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=64, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TensorDataset object for train_data and test_data\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(y_train)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(y_test)\n",
    "\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "test_data = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DataLoader for model training\n",
    "batch_size= 30\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, \\\n",
    "                         sampler = RandomSampler(train_data))\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size,\\\n",
    "                        sampler = SequentialSampler(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\",\\\n",
    "                                                      num_labels =3,\\\n",
    "                                                     output_attentions = False,\\\n",
    "                                                     output_hidden_states = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training took some time, so I saved the trained models at each epoch into a file and loaded the lastest model to continue training...\n",
    "\n",
    "### Loading the latest model to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the latest model to continue training\n",
    "model.load_state_dict(torch.load(\"data_volume/finetuned_Bert_epoch_5.model\",\\\n",
    "                              map_location = torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an Adam optimiser for model training\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps = 1e-8)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\\\n",
    "                                           num_training_steps = len(train_loader)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions for the metrics to show during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to show when training\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds = np.argmax(preds, axis=1).flatten()\n",
    "    labels = labels.flatten()\n",
    "    return f1_score(labels, preds, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds = np.argmax(preds, axis=1).flatten()\n",
    "    labels= labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels):\n",
    "        y_preds = preds[labels==label]\n",
    "        y_true = labels[labels==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model evaluation function\n",
    "\n",
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "def evaluate(val_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        batch = tuple(x.to(device) for x in batch)\n",
    "        \n",
    "        inputs = {\"input_ids\": batch[0],\n",
    "                  \"attention_mask\": batch[1],\n",
    "                 \"labels\": batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val += loss.item()\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs[\"labels\"].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_avg = loss_val / len(val_loader)\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis =0)\n",
    "    true_vals = np.concatenate(true_vals, axis = 0)\n",
    "    \n",
    "    return loss_avg, predictions, true_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1539b35388bb45e8a34cced358dbf51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 6'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Training loss: 0.12146943404028814\n",
      "Validation loss: 0.6087845509702509\n",
      "F1 Score (Weighted): 0.8481668994413407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 7'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Training loss: 0.07777264921120756\n",
      "Validation loss: 0.7050655077804219\n",
      "F1 Score (Weighted): 0.8450843614281871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 8'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Training loss: 0.049247221223292094\n",
      "Validation loss: 0.7406000440770929\n",
      "F1 Score (Weighted): 0.8481822362370254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 9'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Training loss: 0.0414903929195134\n",
      "Validation loss: 0.7783354629169811\n",
      "F1 Score (Weighted): 0.8412518697985188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(6, 10)):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress = tqdm(train_loader, desc = \"Epoch {:1d}\".format(epoch),\\\n",
    "                   disable = False, leave=False)\n",
    "    for batch in progress:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(x.to(device) for x in batch)\n",
    "\n",
    "        inputs = {\"input_ids\": batch[0],\n",
    "                  \"attention_mask\": batch[1],\n",
    "                 \"labels\": batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress.set_postfix({'training_loss:': \"{:.3f}\".format(loss.item()/len(batch))})\n",
    "        \n",
    "    torch.save(model.state_dict(), f\"data_volume/finetuned_Bert_epoch_{epoch}.model\")\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = train_loss/len(train_loader)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    loss_val_avg, predictions, true_vals = evaluate(test_loader)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {loss_val_avg}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val, predictions, true_vals = evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_argmax = [x.argmax() for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.840625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, test_pred_argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Model does quite well. Performs the best compared to Naive Bayes and VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df = pd.read_csv(\"news_headlines_test_sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a predict function that takes in the submission DataLoader\n",
    "def predict(model, subm_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for batch in subm_loader:\n",
    "        batch = tuple(x.to(device) for x in batch)\n",
    "        \n",
    "        inputs = {\"input_ids\": batch[0],\n",
    "                  \"attention_mask\": batch[1]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "       \n",
    "        logits = outputs[0]\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis =0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Daniel/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Preparing the submission text data for our BERT Model\n",
    "encoded_data_subm = tokenizer.batch_encode_plus(\n",
    "    subm_df[\"text\"].values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=64, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_subm = encoded_data_subm['input_ids']\n",
    "attention_masks_subm = encoded_data_subm['attention_mask']\n",
    "subm_data = TensorDataset(input_ids_subm, attention_masks_subm)\n",
    "\n",
    "subm_loader = DataLoader(subm_data, batch_size = batch_size,\\\n",
    "                        sampler = SequentialSampler(subm_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subm_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the prediction with our fine-tuned BERT Model\n",
    "pred = predict(model, subm_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure that we get predictions for all the text in the submission file\n",
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [x.argmax() for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the labels back to the original labels\n",
    "# Negative --> -1\n",
    "# Neutral --> 0\n",
    "# Positive --> 1\n",
    "\n",
    "def id_converter(label):\n",
    "    if label == 0:\n",
    "        return -1\n",
    "    elif label == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the labels for the submission texts\n",
    "final_labels = [id_converter(x) for x in final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df['sentiment'] = final_labels\n",
    "\n",
    "# Exporting\n",
    "subm_df.to_csv('Subm_BERT2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer,RobertaForSequenceClassification\n",
    "from transformers import RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Ġaddition', 'Ġ,', 'Ġa', 'Ġfurther', 'Ġ29', 'Ġemployees', 'Ġcan', 'Ġbe', 'Ġlaid', 'Ġoff', 'Ġuntil', 'Ġfurther', 'Ġnotice', 'Ġand', 'Ġthe', 'Ġwhole', 'Ġworkforce', 'Ġcan', 'Ġbe', 'Ġlaid', 'Ġoff', 'Ġfor', 'Ġshort', 'Ġperiods', 'Ġif', 'Ġneeded', 'Ġ.']\n",
      "[1121, 1285, 2156, 10, 617, 1132, 1321, 64, 28, 4976, 160, 454, 617, 3120, 8, 5, 1086, 6862, 64, 28, 4976, 160, 13, 765, 5788, 114, 956, 479]\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = [tokenizer.tokenize(text) for text in df[\"text\"]]\n",
    "\n",
    "ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]\n",
    "\n",
    "labels = df[\"sentiment\"].values\n",
    "\n",
    "print(tokenized_text[0])\n",
    "\n",
    "print(ids[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "max_len = len(ids[0])\n",
    "for i in ids:\n",
    "    if(len(i)>max_len):\n",
    "        max_len = len(i)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1121  1285  2156 ...     0     0     0]\n",
      " [  133  2730  3258 ...     0     0     0]\n",
      " [  133   923     9 ...     0     0     0]\n",
      " ...\n",
      " [  565  4503  3952 ...     0     0     0]\n",
      " [  642  3894   795 ...     0     0     0]\n",
      " [10643   768  2156 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = pad_sequences(ids, maxlen= max_len, dtype = \"long\",\\\n",
    "                         truncating = \"post\", padding = \"post\")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a label converter (using -1 encounter an error when training)\n",
    "# Negative --> 0\n",
    "# Neutral --> 1\n",
    "# Positive --> 2\n",
    "\n",
    "def label_converter(label):\n",
    "    if label == -1:\n",
    "        return 0\n",
    "    elif label == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels\"] = df[\"sentiment\"].apply(label_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"labels\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training (90%) and validation sets (10%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/Daniel/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Tokenising the text for training and validation sets\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    X_train.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length = 64,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    X_test.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=64, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TensorDataset object for train_data and test_data\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(y_train)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(y_test)\n",
    "\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "test_data = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DataLoader for model training\n",
    "batch_size= 30\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, \\\n",
    "                         sampler = RandomSampler(train_data))\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size,\\\n",
    "                        sampler = SequentialSampler(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained BERT Model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",\\\n",
    "                                                      num_labels =3,\\\n",
    "                                                     output_attentions = False,\\\n",
    "                                                     output_hidden_states = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training took some time, so I saved the trained models at each epoch into a file and loaded the lastest model to continue training...\n",
    "\n",
    "### Loading the latest model to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the latest model to continue training\n",
    "model.load_state_dict(torch.load(\"Roberta_Models/finetuned_RoBerta_epoch_4.model\",\\\n",
    "                              map_location = torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an Adam optimiser for model training\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps = 1e-8)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\\\n",
    "                                           num_training_steps = len(train_loader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to show when training\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds = np.argmax(preds, axis=1).flatten()\n",
    "    labels = labels.flatten()\n",
    "    return f1_score(labels, preds, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds = np.argmax(preds, axis=1).flatten()\n",
    "    labels= labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels):\n",
    "        y_preds = preds[labels==label]\n",
    "        y_true = labels[labels==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "# Model Evaluation Function        \n",
    "def evaluate(val_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        batch = tuple(x.to(device) for x in batch)\n",
    "        \n",
    "        inputs = {\"input_ids\": batch[0],\n",
    "                  \"attention_mask\": batch[1],\n",
    "                 \"labels\": batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val += loss.item()\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs[\"labels\"].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_avg = loss_val / len(val_loader)\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis =0)\n",
    "    true_vals = np.concatenate(true_vals, axis = 0)\n",
    "    \n",
    "    return loss_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd5a019c3384fc9bd4cbcbe0b19d7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 5'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.24292887506696084\n",
      "Validation loss: 0.3852775151079351\n",
      "F1 Score (Weighted): 0.8602593519677375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 6'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Training loss: 0.16111983370501548\n",
      "Validation loss: 0.3760584050958807\n",
      "F1 Score (Weighted): 0.8655368469432009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 7'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Training loss: 0.10889414374832995\n",
      "Validation loss: 0.48438413034785877\n",
      "F1 Score (Weighted): 0.8628741794619146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 8'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Training loss: 0.08655672648455948\n",
      "Validation loss: 0.5353467139330778\n",
      "F1 Score (Weighted): 0.8505026681239553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 9'), FloatProgress(value=0.0, max=96.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Training loss: 0.07298575523115385\n",
      "Validation loss: 0.5353467139330778\n",
      "F1 Score (Weighted): 0.8505026681239553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(5, 10)):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress = tqdm(train_loader, desc = \"Epoch {:1d}\".format(epoch),\\\n",
    "                   disable = False, leave=False)\n",
    "    for batch in progress:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(x.to(device) for x in batch)\n",
    "\n",
    "        inputs = {\"input_ids\": batch[0],\n",
    "                  \"attention_mask\": batch[1],\n",
    "                 \"labels\": batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress.set_postfix({'training_loss:': \"{:.3f}\".format(loss.item()/len(batch))})\n",
    "        \n",
    "    torch.save(model.state_dict(), f\"Roberta_Models/finetuned_RoBerta_epoch_{epoch}.model\")\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = train_loss/len(train_loader)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    loss_val_avg, predictions, true_vals = evaluate(test_loader)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {loss_val_avg}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val, predictions, true_vals = evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_argmax = [x.argmax() for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, test_pred_argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa Model does performs relatively well too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model that seems to have the lowest validation loss and higest F1 score\n",
    "model.load_state_dict(torch.load(\"Roberta_Models/finetuned_RoBerta_epoch_7.model\",\\\n",
    "                              map_location = torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df = pd.read_csv(\"news_headlines_test_sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Daniel/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Preparing the submission text data for our BERT Model\n",
    "encoded_data_subm = tokenizer.batch_encode_plus(\n",
    "    subm_df[\"text\"].values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=64, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_subm = encoded_data_subm['input_ids']\n",
    "attention_masks_subm = encoded_data_subm['attention_mask']\n",
    "subm_data = TensorDataset(input_ids_subm, attention_masks_subm)\n",
    "\n",
    "subm_loader = DataLoader(subm_data, batch_size = batch_size,\\\n",
    "                        sampler = SequentialSampler(subm_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subm_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the prediction with our fine-tuned BERT Model\n",
    "pred = predict(model, subm_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure that we get predictions for all the text in the submission file\n",
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [x.argmax() for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the labels for the submission texts\n",
    "final_labels = [id_converter(x) for x in final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df['sentiment'] = final_labels\n",
    "\n",
    "# Exporting\n",
    "subm_df.to_csv('Subm_RoBERTa3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
